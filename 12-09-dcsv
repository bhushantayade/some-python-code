SELECT 
    file_name,
    REGEXP_EXTRACT(file_name, r'^(?:\d+_)?([a-zA-Z0-9]+(?:_[a-zA-Z0-9]+)*)(?:_\d+)?\.[^.]+$') AS base_file_name
FROM file_table;

^(?:[^_]+_)?([^_.]+)(?:_[^_.]+)?\.[^.]+$


SELECT 
    file_name,
    REGEXP_EXTRACT(file_name, r'^(?:[^_]+_)?([^_.]+)(?:_[^_.]+)?\.[^.]+$') AS base_file_name
FROM file_table;



CREATE OR REPLACE VIEW file_occurrence_view AS
WITH Extracted_Files AS (
    SELECT 
        bucket_name,
        folder_name,
        file_name,
        create_time,
        REGEXP_EXTRACT(file_name, r'^[^-]+-(.+)-[^-]+\..+$') AS base_file_name 
    FROM file_table
),
File_Difference AS (
    SELECT 
        ef.bucket_name,
        ef.folder_name,
        ef.file_name,
        ef.create_time,
        ef.base_file_name,
        LAG(ef.create_time) OVER (
            PARTITION BY ef.bucket_name, ef.folder_name, ef.base_file_name 
            ORDER BY ef.create_time
        ) AS previous_create_time
    FROM Extracted_Files ef
),
Final_Calculation AS (
    SELECT 
        fd.bucket_name,
        fd.folder_name,
        fd.file_name,
        fd.create_time,
        fd.base_file_name,
        fd.previous_create_time,
        f.frequency,
        -- Convert frequency to seconds for calculation
        CASE 
            WHEN f.frequency = 'daily' THEN 86400  -- 24 hours = 86400 seconds
            WHEN f.frequency = 'hourly' THEN 3600  -- 1 hour = 3600 seconds
            WHEN f.frequency = 'every 10 min' THEN 600  -- 10 min = 600 seconds
            ELSE NULL
        END AS expected_time_diff_seconds,
        -- Actual time difference
        TIMESTAMP_DIFF(fd.create_time, fd.previous_create_time, SECOND) AS actual_time_diff_seconds,
        -- Determine if file is late
        CASE 
            WHEN fd.previous_create_time IS NOT NULL 
                 AND TIMESTAMP_DIFF(fd.create_time, fd.previous_create_time, SECOND) > 
                     CASE 
                         WHEN f.frequency = 'daily' THEN 86400
                         WHEN f.frequency = 'hourly' THEN 3600
                         WHEN f.frequency = 'every 10 min' THEN 600
                         ELSE NULL
                     END 
            THEN 'Late'
            ELSE 'On Time'
        END AS status
    FROM File_Difference fd
    LEFT JOIN frequency_table f 
    ON fd.bucket_name = f.bucket_name 
    AND fd.folder_name = f.folder_name
)
SELECT * FROM Final_Calculation;






Subject: Reminder: Your File Will Be Deleted in 24 Hours

Dear [User's Name],

We hope you’re doing well. This is a friendly reminder that your file [File Name] stored in [Bucket Name] will be automatically deleted in 24 hours as part of our retention policy.

If you need to retain this file, please take the necessary action before it is permanently removed.

For any questions or assistance, feel free to reach out to us.

Best regards,
[Your Name / Team Name]




github.com/kss7/Pytest-Automation
###
pytest test_main.py


import pytest
from main import validate_file

def test_valid_file():
    file_content = '{"name": "John Doe", "age": 30}'
    result = validate_file(file_content)
    assert result == {"status": "success", "message": "File is valid"}

def test_invalid_json():
    file_content = '{"name": "John Doe", "age": 30'  # Missing closing brace
    result = validate_file(file_content)
    assert result == {"status": "error", "message": "Invalid JSON format"}

def test_invalid_name():
    file_content = '{"name": 123, "age": 30}'
    result = validate_file(file_content)
    assert result == {"status": "error", "message": "Invalid name format"}

def test_invalid_age():
    file_content = '{"name": "John Doe", "age": "thirty"}'
    result = validate_file(file_content)
    assert result == {"status": "error", "message": "Invalid age format"}



#######
import re

text = "Header2025-01-220000006873"  # Example string
match = re.search(r"\d{8}(\d+)$", text)  # Match digits after ddhhmmss

if match:
    print(match.group(1))  # Output: 6873





import pandas as pd
import csv
from google.cloud import storage

def detect_delimiter_from_blob(blob):
    """Detects the delimiter from the first line of a GCS blob."""
    first_line = blob.download_as_text().split("\n", 1)[0]  # Read only the first line
    return csv.Sniffer().sniff(first_line).delimiter

def check_column_and_count_rows(bucket_name, file_path, column_name, chunk_size=10000):
    try:
        # Initialize GCS client
        client = storage.Client()
        bucket = client.bucket(bucket_name)
        blob = bucket.blob(file_path)

        if not blob.exists():
            print("File does not exist in GCS.")
            return
        
        # Detect delimiter
        delimiter = detect_delimiter_from_blob(blob)
        print(f"Detected delimiter: '{delimiter}'")

        # Read the first line (header) to check column existence
        first_line = blob.download_as_text().split("\n", 1)[0]
        headers = first_line.strip().split(delimiter)

        if column_name not in headers:
            print(f"Column '{column_name}' not found in the file.")
            return

        # Read file in chunks
        row_count = 0
        with blob.open("rt") as file:  # Read as text stream
            next(file)  # Skip header
            chunk_iterator = pd.read_csv(file, sep=delimiter, skiprows=0, chunksize=chunk_size, header=None, low_memory=False)

            for chunk in chunk_iterator:
                row_count += len(chunk)

        print(f"Column '{column_name}' found. Row count (excluding header): {row_count}")

    except Exception as e:
        print(f"Error processing file: {e}")

# Example usage
bucket_name = "your-bucket-name"
file_path = "your-folder/data.dat"
column_name = "your_column_name"
check_column_and_count_rows(bucket_name, file_path, column_name)




import pandas as pd
import csv

def detect_delimiter(file_path):
    """Detects the delimiter of the given .dat file."""
    with open(file_path, 'r', encoding='utf-8') as file:
        first_line = file.readline()
        return csv.Sniffer().sniff(first_line).delimiter

def check_column_and_count_rows(file_path, column_name, chunk_size=10000):
    try:
        # Detect delimiter
        delimiter = detect_delimiter(file_path)
        print(f"Detected delimiter: '{delimiter}'")

        # Read only the first line (header) to check column existence
        with open(file_path, 'r', encoding='utf-8') as file:
            headers = file.readline().strip().split(delimiter)

        if column_name not in headers:
            print(f"Column '{column_name}' not found in the file.")
            return

        # Initialize row count
        row_count = 0

        # Read file in chunks, skipping the header row
        chunk_iterator = pd.read_csv(file_path, sep=delimiter, skiprows=1, chunksize=chunk_size, header=None, low_memory=False)

        for chunk in chunk_iterator:
            row_count += len(chunk)

        print(f"Column '{column_name}' found. Row count (excluding header): {row_count}")

    except Exception as e:
        print(f"Error reading file: {e}")

# Example usage
file_path = "data.dat"
column_name = "your_column_name"
check_column_and_count_rows(file_path, column_name)









Subject: Diwali Leave Request

Dear [Manager's Name],

I hope this message finds you well. I would like to request leave from [start date] to [end date] to celebrate Diwali with my family. I’ve planned my tasks to ensure a smooth workflow during my absence.

Thank you for considering my request.

Best regards,
[Your Name]


import smtplib
import socket
from smtplib import SMTPException
import os

def send_email(request):
    try:
        smtp_server = 'smtp.example.com'
        sender_email = os.getenv('SENDER_EMAIL')
        receiver_email = os.getenv('RECEIVER_EMAIL')
        message = 'Subject: Test Email\n\nThis is a test email from Google Cloud Function.'

        # Setting timeout and handling it
        with smtplib.SMTP(smtp_server, 587, timeout=10) as server:
            server.starttls()
            server.login(os.getenv('SMTP_USER'), os.getenv('SMTP_PASSWORD'))
            server.sendmail(sender_email, receiver_email, message)
            return "Email sent successfully", 200

    except socket.timeout:
        return "Error: Connection to the SMTP server timed out.", 500

    except SMTPException as e:
        return f"SMTP error occurred: {e}", 500

    except Exception as e:
        return f"An error occurred: {e}", 500

import os
from sendgrid import SendGridAPIClient
from sendgrid.helpers.mail import Mail

def send_email(request):
    try:
        message = Mail(
            from_email=os.getenv('SENDER_EMAIL'),
            to_emails=os.getenv('RECEIVER_EMAIL'),
            subject='Test Email from Cloud Function',
            plain_text_content='This is a test email.'
        )
        sg = SendGridAPIClient(os.getenv('SENDGRID_API_KEY'))
        response = sg.send(message)
        return "Email sent successfully", 200

    except Exception as e:
        return f"An error occurred: {e}", 500


SELECT *
FROM status_table s
WHERE s.status = 'fail'
AND NOT EXISTS (
    SELECT 1
    FROM status_table s2
    WHERE s2.id = s.id
    AND s2.status = 'pass'
    AND s2.timestamp = (
        SELECT MAX(s3.timestamp)
        FROM status_table s3
        WHERE s3.id = s.id
    )
);


SELECT 
    column1,                -- A column that you want to group by
    column2,                -- Another column for aggregation
    ARRAY_AGG(text_column ORDER BY timestamp_column DESC LIMIT 1)[OFFSET(0)] AS latest_text_column
FROM your_table
GROUP BY column1, column2;


SELECT 
  main_table.date,
  
  -- Count of successful file IDs
  COUNT(CASE WHEN status = 'success' THEN file_id END) AS success_count,
  
  -- Count of failed file IDs
  COUNT(CASE WHEN status = 'fail' THEN file_id END) AS fail_count,
  
  -- Comma-separated list of successful file IDs
  STRING_AGG(CASE WHEN status = 'success' THEN file_id END, ', ') AS success_file_ids,
  
  -- Comma-separated list of failed file IDs
  STRING_AGG(CASE WHEN status = 'fail' THEN file_id END, ', ') AS fail_file_ids
  
FROM 
  main_table
JOIN 
  file_status_table f ON main_table.file_id = f.file_id -- Example join with file_status_table
  
GROUP BY 
  main_table.date
ORDER BY 
  main_table.date;


SELECT t1.*, t2.*
FROM table1 t1
JOIN table2 t2
ON SPLIT(t1.path_column, '/')[OFFSET(LENGTH(SPLIT(t1.path_column, '/')) - 1)] = t2.file_name -- always join on file_name
AND (
    -- Only join on folder_name if folder_name is present (path has more than 1 segment before the file_name)
    CASE 
        WHEN ARRAY_LENGTH(SPLIT(t1.path_column, '/')) > 2 THEN SPLIT(t1.path_column, '/')[OFFSET(LENGTH(SPLIT(t1.path_column, '/')) - 2)] = t2.folder_name
        ELSE TRUE -- skip folder join condition
    END
)



SELECT *
FROM table1 t1
JOIN table2 t2 
ON t1.path = CONCAT('project/bucket_name/', t2.folder_name, '/', t2.file_name)




SELECT *
FROM table1 t1
JOIN table2 t2 
ON SUBSTRING_INDEX(t1.path, '/', -1) = t2.file_name




SELECT *
FROM table1 t1
JOIN table2 t2 
ON t1.path LIKE CONCAT('%/', t2.file_name) 
   AND t1.path LIKE CONCAT('%/', t2.folder_name, '/%')




from google.cloud import storage

# Initialize GCS client
client = storage.Client()

# Define your bucket and folder name
bucket_name = "your-bucket-name"  # Replace with your actual bucket name
folder_name = "folder99/"

# Get the bucket
bucket = client.get_bucket(bucket_name)

# Check if the folder exists
blobs = bucket.list_blobs(prefix=folder_name, delimiter="/")

# Checking if any blobs (files) or subfolders exist under the folder
if any(blobs):
    print(f"The folder '{folder_name}' exists in bucket '{bucket_name}'.")
else:
    print(f"The folder '{folder_name}' does not exist in bucket '{bucket_name}'.")



To achieve this workflow, you can follow these steps to trigger a Google Cloud Function when a new row is inserted into a BigQuery table, validate the details, and log the result (pass/fail) into another table:

Steps:
Create a Pub/Sub Topic for BigQuery Insertions:

Use BigQuery’s Data Change Notifications (DCN) to automatically publish a message to a Pub/Sub topic whenever a new row is inserted.
First, enable data change notification for your BigQuery table:
bash
Copy code
bq update --set-async-notification-config=topic=<your-pubsub-topic> <your-dataset>.<your-table>
Create a Pub/Sub Subscription:

Create a subscription for the Pub/Sub topic you configured.
bash
Copy code
gcloud pubsub subscriptions create <your-subscription> --topic=<your-pubsub-topic>
Write a Google Cloud Function to Validate Data:

Set up a Google Cloud Function to be triggered by the Pub/Sub topic. This function will receive messages whenever a new row is inserted.
Inside the function, you can validate the inserted row's data.
Make Entry in the Pass/Fail Table:

Based on the validation logic, you can write the result (pass or fail) into another BigQuery table.
Code Example:
1. Pub/Sub Triggered Cloud Function:
python
Copy code
import json
import base64
from google.cloud import bigquery

def validate_data(data):
    # Add your custom validation logic here
    return True if "some_condition" in data else False

def insert_pass_fail_entry(bq_client, dataset, table, file_id, status):
    table_ref = bq_client.dataset(dataset).table(table)
    row_to_insert = [
        {u"file_id": file_id, u"validation_status": status}
    ]
    errors = bq_client.insert_rows_json(table_ref, row_to_insert)
    if errors:
        print(f"Failed to insert row: {errors}")
    else:
        print("Row inserted successfully.")

def process_pubsub_event(event, context):
    # Parse Pub/Sub message
    pubsub_message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(pubsub_message)
    
    # Extract relevant information
    file_id = data.get("file_id")  # Assuming this field is in your table
    
    # Initialize BigQuery client
    bq_client = bigquery.Client()
    
    # Validate the row data
    is_valid = validate_data(data)
    
    # Insert validation result into another table
    validation_status = "PASS" if is_valid else "FAIL"
    insert_pass_fail_entry(bq_client, 'your_dataset', 'your_validation_table', file_id, validation_status)
    
    return 'Processing complete.'
Steps to Deploy:
Create the Cloud Function:

bash
Copy code
gcloud functions deploy validateBigQueryRow \
--runtime python310 \
--trigger-topic <your-pubsub-topic> \
--entry-point process_pubsub_event \
--region <your-region>
Validation Logic:

In the validate_data function, add any specific business logic required for validation.
Pass/Fail Table:

Ensure that the your_validation_table table has the necessary columns like file_id and validation_status to store the results.
Workflow Explanation:
New Row Insertion: Whenever a new row is inserted into the target BigQuery table, a message is sent to the configured Pub/Sub topic.
Cloud Function Triggered: The Pub/Sub message triggers the Google Cloud Function.
Validation: The function extracts the row data, validates it, and inserts a pass or fail result into another BigQuery table.
This setup provides a scalable and serverless solution for automating row validation and logging in BigQuery.
