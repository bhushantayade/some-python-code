import smtplib
import socket
from smtplib import SMTPException
import os

def send_email(request):
    try:
        smtp_server = 'smtp.example.com'
        sender_email = os.getenv('SENDER_EMAIL')
        receiver_email = os.getenv('RECEIVER_EMAIL')
        message = 'Subject: Test Email\n\nThis is a test email from Google Cloud Function.'

        # Setting timeout and handling it
        with smtplib.SMTP(smtp_server, 587, timeout=10) as server:
            server.starttls()
            server.login(os.getenv('SMTP_USER'), os.getenv('SMTP_PASSWORD'))
            server.sendmail(sender_email, receiver_email, message)
            return "Email sent successfully", 200

    except socket.timeout:
        return "Error: Connection to the SMTP server timed out.", 500

    except SMTPException as e:
        return f"SMTP error occurred: {e}", 500

    except Exception as e:
        return f"An error occurred: {e}", 500

import os
from sendgrid import SendGridAPIClient
from sendgrid.helpers.mail import Mail

def send_email(request):
    try:
        message = Mail(
            from_email=os.getenv('SENDER_EMAIL'),
            to_emails=os.getenv('RECEIVER_EMAIL'),
            subject='Test Email from Cloud Function',
            plain_text_content='This is a test email.'
        )
        sg = SendGridAPIClient(os.getenv('SENDGRID_API_KEY'))
        response = sg.send(message)
        return "Email sent successfully", 200

    except Exception as e:
        return f"An error occurred: {e}", 500


SELECT *
FROM status_table s
WHERE s.status = 'fail'
AND NOT EXISTS (
    SELECT 1
    FROM status_table s2
    WHERE s2.id = s.id
    AND s2.status = 'pass'
    AND s2.timestamp = (
        SELECT MAX(s3.timestamp)
        FROM status_table s3
        WHERE s3.id = s.id
    )
);


SELECT 
    column1,                -- A column that you want to group by
    column2,                -- Another column for aggregation
    ARRAY_AGG(text_column ORDER BY timestamp_column DESC LIMIT 1)[OFFSET(0)] AS latest_text_column
FROM your_table
GROUP BY column1, column2;


SELECT 
  main_table.date,
  
  -- Count of successful file IDs
  COUNT(CASE WHEN status = 'success' THEN file_id END) AS success_count,
  
  -- Count of failed file IDs
  COUNT(CASE WHEN status = 'fail' THEN file_id END) AS fail_count,
  
  -- Comma-separated list of successful file IDs
  STRING_AGG(CASE WHEN status = 'success' THEN file_id END, ', ') AS success_file_ids,
  
  -- Comma-separated list of failed file IDs
  STRING_AGG(CASE WHEN status = 'fail' THEN file_id END, ', ') AS fail_file_ids
  
FROM 
  main_table
JOIN 
  file_status_table f ON main_table.file_id = f.file_id -- Example join with file_status_table
  
GROUP BY 
  main_table.date
ORDER BY 
  main_table.date;


SELECT t1.*, t2.*
FROM table1 t1
JOIN table2 t2
ON SPLIT(t1.path_column, '/')[OFFSET(LENGTH(SPLIT(t1.path_column, '/')) - 1)] = t2.file_name -- always join on file_name
AND (
    -- Only join on folder_name if folder_name is present (path has more than 1 segment before the file_name)
    CASE 
        WHEN ARRAY_LENGTH(SPLIT(t1.path_column, '/')) > 2 THEN SPLIT(t1.path_column, '/')[OFFSET(LENGTH(SPLIT(t1.path_column, '/')) - 2)] = t2.folder_name
        ELSE TRUE -- skip folder join condition
    END
)



SELECT *
FROM table1 t1
JOIN table2 t2 
ON t1.path = CONCAT('project/bucket_name/', t2.folder_name, '/', t2.file_name)




SELECT *
FROM table1 t1
JOIN table2 t2 
ON SUBSTRING_INDEX(t1.path, '/', -1) = t2.file_name




SELECT *
FROM table1 t1
JOIN table2 t2 
ON t1.path LIKE CONCAT('%/', t2.file_name) 
   AND t1.path LIKE CONCAT('%/', t2.folder_name, '/%')




from google.cloud import storage

# Initialize GCS client
client = storage.Client()

# Define your bucket and folder name
bucket_name = "your-bucket-name"  # Replace with your actual bucket name
folder_name = "folder99/"

# Get the bucket
bucket = client.get_bucket(bucket_name)

# Check if the folder exists
blobs = bucket.list_blobs(prefix=folder_name, delimiter="/")

# Checking if any blobs (files) or subfolders exist under the folder
if any(blobs):
    print(f"The folder '{folder_name}' exists in bucket '{bucket_name}'.")
else:
    print(f"The folder '{folder_name}' does not exist in bucket '{bucket_name}'.")



To achieve this workflow, you can follow these steps to trigger a Google Cloud Function when a new row is inserted into a BigQuery table, validate the details, and log the result (pass/fail) into another table:

Steps:
Create a Pub/Sub Topic for BigQuery Insertions:

Use BigQueryâ€™s Data Change Notifications (DCN) to automatically publish a message to a Pub/Sub topic whenever a new row is inserted.
First, enable data change notification for your BigQuery table:
bash
Copy code
bq update --set-async-notification-config=topic=<your-pubsub-topic> <your-dataset>.<your-table>
Create a Pub/Sub Subscription:

Create a subscription for the Pub/Sub topic you configured.
bash
Copy code
gcloud pubsub subscriptions create <your-subscription> --topic=<your-pubsub-topic>
Write a Google Cloud Function to Validate Data:

Set up a Google Cloud Function to be triggered by the Pub/Sub topic. This function will receive messages whenever a new row is inserted.
Inside the function, you can validate the inserted row's data.
Make Entry in the Pass/Fail Table:

Based on the validation logic, you can write the result (pass or fail) into another BigQuery table.
Code Example:
1. Pub/Sub Triggered Cloud Function:
python
Copy code
import json
import base64
from google.cloud import bigquery

def validate_data(data):
    # Add your custom validation logic here
    return True if "some_condition" in data else False

def insert_pass_fail_entry(bq_client, dataset, table, file_id, status):
    table_ref = bq_client.dataset(dataset).table(table)
    row_to_insert = [
        {u"file_id": file_id, u"validation_status": status}
    ]
    errors = bq_client.insert_rows_json(table_ref, row_to_insert)
    if errors:
        print(f"Failed to insert row: {errors}")
    else:
        print("Row inserted successfully.")

def process_pubsub_event(event, context):
    # Parse Pub/Sub message
    pubsub_message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(pubsub_message)
    
    # Extract relevant information
    file_id = data.get("file_id")  # Assuming this field is in your table
    
    # Initialize BigQuery client
    bq_client = bigquery.Client()
    
    # Validate the row data
    is_valid = validate_data(data)
    
    # Insert validation result into another table
    validation_status = "PASS" if is_valid else "FAIL"
    insert_pass_fail_entry(bq_client, 'your_dataset', 'your_validation_table', file_id, validation_status)
    
    return 'Processing complete.'
Steps to Deploy:
Create the Cloud Function:

bash
Copy code
gcloud functions deploy validateBigQueryRow \
--runtime python310 \
--trigger-topic <your-pubsub-topic> \
--entry-point process_pubsub_event \
--region <your-region>
Validation Logic:

In the validate_data function, add any specific business logic required for validation.
Pass/Fail Table:

Ensure that the your_validation_table table has the necessary columns like file_id and validation_status to store the results.
Workflow Explanation:
New Row Insertion: Whenever a new row is inserted into the target BigQuery table, a message is sent to the configured Pub/Sub topic.
Cloud Function Triggered: The Pub/Sub message triggers the Google Cloud Function.
Validation: The function extracts the row data, validates it, and inserts a pass or fail result into another BigQuery table.
This setup provides a scalable and serverless solution for automating row validation and logging in BigQuery.
