import re

def reverse_resolve_date_path(file_path: str) -> str:
    """
    Reverse-engineer file paths by replacing detected date substrings 
    with generic placeholders (YYYY, YY, MM, DD, YYYYMM, YYYYMMDD).
    """

    # Replace YYYYMMDD first (most specific: 8 digits)
    file_path = re.sub(r'\b\d{8}\b', 'YYYYMMDD', file_path)

    # Replace YYYYMM (6 digits) but not overlapping with YYYYMMDD
    file_path = re.sub(r'\b\d{6}\b', 'YYYYMM', file_path)

    # Replace YYYY (4 digits, usually years between 1900–2099)
    file_path = re.sub(r'\b(19|20)\d{2}\b', 'YYYY', file_path)

    # Replace YY (2 digits) carefully, avoid touching parts of larger numbers
    file_path = re.sub(r'\b\d{2}\b', 'YY', file_path)

    # Replace standalone MM (01–12)
    file_path = re.sub(r'\b(0[1-9]|1[0-2])\b', 'MM', file_path)

    # Replace standalone DD (01–31)
    file_path = re.sub(r'\b(0[1-9]|[12]\d|3[01])\b', 'DD', file_path)

    return file_path



#####
from google.cloud import bigquery_storage_v1
from google.cloud.bigquery_storage_v1 import types
from google.protobuf.json_format import ParseDict
import uuid


def insert_rows(project_id: str, dataset: str, table: str, rows: list[dict], stream_type: str = "COMMITTED"):
    """
    Insert rows into BigQuery using the Storage Write API.
    Args:
        project_id (str): GCP Project ID
        dataset (str): BigQuery Dataset
        table (str): BigQuery Table
        rows (list[dict]): List of rows to insert
        stream_type (str): "COMMITTED" (streaming) or "PENDING" (batch)
    """

    client = bigquery_storage_v1.BigQueryWriteClient()
    table_path = client.table_path(project_id, dataset, table)

    # Create a write stream
    write_stream = types.WriteStream(type_=getattr(types.WriteStream.Type, stream_type))
    stream = client.create_write_stream(parent=table_path, write_stream=write_stream)

    # Build the AppendRowsRequest
    proto_schema = client.get_write_stream(name=stream.name).table_schema
    request_template = types.AppendRowsRequest(
        write_stream=stream.name,
        proto_rows=types.AppendRowsRequest.ProtoData(
            writer_schema=types.ProtoSchema(proto_descriptor=proto_schema.proto_descriptor)
        ),
    )

    # Fill in rows
    proto_rows = types.ProtoRows()
    for row in rows:
        # Add deduplication ID
        row["_insert_id"] = str(uuid.uuid4())
        proto_row = ParseDict(row, types.ProtoRows().rows.add())  # convert dict -> proto
        proto_rows.rows.append(proto_row)

    # Send request
    request = types.AppendRowsRequest(
        write_stream=stream.name,
        proto_rows=types.AppendRowsRequest.ProtoData(rows=proto_rows),
    )
    response = client.append_rows(iter([request]))

    # Finalize batch streams
    if stream_type == "PENDING":
        client.finalize_write_stream(name=stream.name)

    return f"✅ Inserted {len(rows)} rows into {dataset}.{table} with {stream_type} stream."


# -------------------
# Example usage
# -------------------
if __name__ == "__main__":
    PROJECT = "your_project"
    DATASET = "your_dataset"
    TABLE = "your_table"

    test_rows = [
        {"id": 1, "name": "Alice"},
        {"id": 2, "name": "Bob"},
    ]

    print(insert_rows(PROJECT, DATASET, TABLE, test_rows, stream_type="COMMITTED"))





#######
from google.cloud import bigquery_storage_v1
from google.cloud.bigquery_storage_v1 import types
import uuid


def insert_rows(project_id: str, dataset: str, table: str, rows: list[dict], stream_type: str = "COMMITTED"):
    """
    Insert rows into BigQuery using the Storage Write API (JsonStreamWriter).
    """

    client = bigquery_storage_v1.BigQueryWriteClient()
    table_path = client.table_path(project_id, dataset, table)

    # Create write stream
    write_stream = types.WriteStream()
    write_stream.type_ = getattr(types.WriteStream.Type, stream_type)
    stream = client.create_write_stream(parent=table_path, write_stream=write_stream)

    # JSON stream writer
    writer = bigquery_storage_v1.JsonStreamWriter(
        client=client,
        stream_name=stream.name,
    )

    # Add insert IDs (optional but recommended for deduplication)
    for row in rows:
        row["_insert_id"] = str(uuid.uuid4())

    # Append rows
    writer.append(rows)
    writer.close()

    if stream_type == "PENDING":
        client.finalize_write_stream(name=stream.name)

    return f"✅ Inserted {len(rows)} rows into {dataset}.{table} using {stream_type} stream."


# -----------------------
# Quick Test Run
# -----------------------
if __name__ == "__main__":
    PROJECT = "your_project"
    DATASET = "your_dataset"
    TABLE = "your_table"

    test_rows = [
        {"id": 1, "name": "Alice"},
        {"id": 2, "name": "Bob"},
    ]

    print(insert_rows(PROJECT, DATASET, TABLE, test_rows, stream_type="COMMITTED"))


#####
from google.cloud import bigquery_storage_v1
from google.api_core.retry import Retry
import json
import uuid


class BigQueryWriter:
    def __init__(self, project_id: str, dataset: str, table: str):
        self.client = bigquery_storage_v1.BigQueryWriteClient()
        self.parent = self.client.table_path(project_id, dataset, table)

    def insert_rows(self, rows: list[dict], stream_type: str = "COMMITTED"):
        """
        Insert rows into BigQuery using the Storage Write API.

        Args:
            rows (list[dict]): List of rows to insert (e.g., [{"id": 1, "name": "Alice"}]).
            stream_type (str): "COMMITTED" (default) for real-time,
                               "PENDING" for batch load.
        """
        # Create write stream (COMMITTED = visible immediately)
        write_stream = bigquery_storage_v1.WriteStream()
        write_stream.type_ = getattr(bigquery_storage_v1.WriteStream.Type, stream_type)
        stream = self.client.create_write_stream(parent=self.parent, write_stream=write_stream)

        # Prepare rows
        proto_rows = bigquery_storage_v1.types.ProtoRows()
        for row in rows:
            # Assign unique insert_id for deduplication
            row["_insert_id"] = str(uuid.uuid4())
            proto_rows.serialized_rows.append(json.dumps(row).encode("utf-8"))

        # Create request
        request = bigquery_storage_v1.AppendRowsRequest(
            write_stream=stream.name,
            proto_rows=bigquery_storage_v1.AppendRowsRequest.ProtoData(
                rows=proto_rows
            )
        )

        # Append rows with retry
        response = self.client.append_rows(
            iter([request]),
            retry=Retry(deadline=30.0, maximum=3)  # retry on transient errors
        )

        # Finalize the stream for batch loads
        if stream_type == "PENDING":
            self.client.finalize_write_stream(name=stream.name)

        return response


# -------------------
# ✅ Usage Example
# -------------------
if __name__ == "__main__":
    writer = BigQueryWriter("your_project", "your_dataset", "your_table")

    # Real-time streaming insert
    rows = [
        {"id": 1, "name": "Alice"},
        {"id": 2, "name": "Bob"}
    ]
    response = writer.insert_rows(rows, stream_type="COMMITTED")
    print("Streaming insert done:", response)

    # Batch insert (not visible until committed later)
    batch_rows = [
        {"id": 3, "name": "Charlie"},
        {"id": 4, "name": "David"}
    ]
    response = writer.insert_rows(batch_rows, stream_type="PENDING")
    print("Batch insert done:", response)


#####
WITH parent AS (
  SELECT
    bucket_name,
    file_path AS original_path
  FROM `project.dataset.parent_logs`
),
resolved AS (
  SELECT
    bucket_name,
    resolved_path
  FROM `project.dataset.resolved_paths`
)

SELECT
  p.bucket_name,
  p.original_path,
  r.resolved_path
FROM parent p
JOIN resolved r
  ON p.bucket_name = r.bucket_name
 AND REGEXP_REPLACE(
       p.original_path,
       r'YYYYMMDD|yyyyMMdd|YYYYMM|yyyyMM|YYYY|yyyy|YY|yy|MM|mm|DD|dd',
       ''
     ) = REGEXP_REPLACE(
       r.resolved_path,
       r'[0-9]{8}|[0-9]{6}|[0-9]{4}|[0-9]{2}',
       ''
     );


#####
import re
from datetime import datetime

def resolve_date_path(file_path: str, target_date: datetime = None) -> str:
    """
    Replace date placeholders safely, avoiding replacements embedded within alphabetic words.
    Supports both uppercase and lowercase tokens:
      YYYY, YY, MM, DD
      yyyy, yy, mm, dd
    Also supports combined tokens like YYYYMMDD or yyyymmdd.
    """
    if target_date is None:
        target_date = datetime.now()

    # Replacement tokens ordered longest-first
    replacements = [
        ("YYYYMMDD", target_date.strftime("%Y%m%d")),
        ("YYYYMM",   target_date.strftime("%Y%m")),
        ("YYYY",     target_date.strftime("%Y")),
        ("YY",       target_date.strftime("%y")),
        ("MM",       target_date.strftime("%m")),
        ("DD",       target_date.strftime("%d")),

        ("yyyyMMdd", target_date.strftime("%Y%m%d")),  # lowercase full
        ("yyyyMM",   target_date.strftime("%Y%m")),
        ("yyyy",     target_date.strftime("%Y")),
        ("yy",       target_date.strftime("%y")),
        ("mm",       target_date.strftime("%m")),
        ("dd",       target_date.strftime("%d")),
    ]

    result = file_path
    for placeholder, replacement in replacements:
        # negative lookbehind/lookahead for alphabetic characters only
        # ensures tokens inside words like COMMUNICATION won't be replaced
        pattern = rf"(?<![A-Za-z]){re.escape(placeholder)}(?![A-Za-z])"
        result = re.sub(pattern, replacement, result)

    return result


------
import re
from datetime import datetime

def resolve_date_path(file_path, target_date=None):
    """
    Safely resolve date placeholders (YYYY, YY, MM, DD, YYYYMM, YYYYMMDD)
    in a file path without replacing random substrings like 'COMMUNICATION'.
    """
    if target_date is None:
        target_date = datetime.now()

    # Ordered longest → shortest to avoid partial replacements
    format_replacements = [
        (r'YYYYMMDD', target_date.strftime('%Y%m%d')),
        (r'YYYYMM', target_date.strftime('%Y%m')),
        (r'YYYY', target_date.strftime('%Y')),
        (r'YY', target_date.strftime('%y')),
        (r'MM', target_date.strftime('%m')),
        (r'DD', target_date.strftime('%d'))
    ]

    # Replace only when they appear as clear tokens or sequences
    for pattern, replacement in format_replacements:
        file_path = re.sub(pattern, replacement, file_path)

    return file_path

TEST_DATE = datetime(2023, 11, 15)

print(resolve_date_path("data/YYYY/file.json", TEST_DATE))    
# → data/2023/file.json

print(resolve_date_path("logs/YYYYMM/app.log", TEST_DATE))   
# → logs/202311/app.log

print(resolve_date_path("exports/YYYYMMDD/data.csv", TEST_DATE))   
# → exports/20231115/data.csv

print(resolve_date_path("archive/YYYY/MM/DD/backup.zip", TEST_DATE))   
# → archive/2023/11/15/backup.zip

print(resolve_date_path("COMMUNICATION_test.txt", TEST_DATE))   
# → COMMUNICATION_test.txt   ✅ untouched

print(resolve_date_path("X_YY_MM_DD_Y", TEST_DATE))  
# → X_23_11_15_Y

print(resolve_date_path("1YYYY2MM3DD4", TEST_DATE))  
# → 1202311154   ✅ fixed




#####
WITH parsed AS (
  SELECT
    resource.labels.bucket_name AS bucket_name,
    -- Extract file_name without generation number
    REGEXP_EXTRACT(protoPayload.resourceName, r'/objects/([^#]+)') AS file_name,
    timestamp AS create_ts
  FROM
    `project.gcs_logs.cloudaudit_googleapis_com_data_access`
  WHERE
    protoPayload.methodName = "storage.objects.create"
    AND protoPayload.resourceName IS NOT NULL
)

SELECT
  bucket_name,
  create_date,
  COUNT(1) AS unique_file_count_first_seen
FROM (
  SELECT
    bucket_name,
    file_name,
    DATE(create_ts) AS create_date,
    ROW_NUMBER() OVER (
      PARTITION BY bucket_name, file_name
      ORDER BY create_ts
    ) AS rn
  FROM parsed
  WHERE file_name IS NOT NULL
)
WHERE rn = 1
GROUP BY bucket_name, create_date
ORDER BY bucket_name, create_date;
