import re
from typing import Tuple, Dict

def reverse_date_path_v4(path: str) -> Tuple[str, Dict[str, str]]:
    """
    Robust reverse function that:
     - detects YYYYMMDD, YYYYMM, YYYY (with plausible years 1900-2099)
     - detects yyyy/mm/dd and yy/mm/dd forms
     - recognizes month= / day= / year= context
     - avoids replacing digits embedded in alphabetic words (e.g. COMMUNICATION)
     - avoids wrongly replacing 2-digit groups inside longer numeric sequences

    Returns:
        (reversed_path, mapping_dict)
        where mapping_dict = {actual_value: placeholder}
    """
    p = path
    mapping = {}

    # 1) contiguous numeric formats (longest-first)
    def repl(match, token):
        val = match.group(1)
        mapping[val] = token
        return token

    p = re.sub(r'(?<![A-Za-z])((?:19|20)\d{6})(?![A-Za-z])', lambda m: repl(m, 'YYYYMMDD'), p)
    p = re.sub(r'(?<![A-Za-z])((?:19|20)\d{4})(?![A-Za-z])', lambda m: repl(m, 'YYYYMM'), p)

    # 2) yyyy/mm/dd or yyyy-mm-dd or yyyy_mm_dd -> YYYY/MM/DD
    p = re.sub(
        r'(?<![A-Za-z])((?:19|20)\d{2})[\/\-_](0[1-9]|1[0-2])[\/\-_](0[1-9]|[12]\d|3[01])(?![A-Za-z])',
        lambda m: _map_seq(m, ['YYYY','MM','DD'], mapping),
        p
    )

    # 3) yy/mm/dd -> YY/MM/DD
    p = re.sub(
        r'(?<![A-Za-z])(\d{2})[\/\-_](0[1-9]|1[0-2])[\/\-_](0[1-9]|[12]\d|3[01])(?![A-Za-z])',
        lambda m: _map_seq(m, ['YY','MM','DD'], mapping),
        p
    )

    # 4) context-aware key=value replacements
    p = re.sub(r'(?i)(month=)(0[1-9]|1[0-2])',
               lambda m: f"{m.group(1)}{_map_val(m.group(2),'MM',mapping)}", p)
    p = re.sub(r'(?i)(day=)(0[1-9]|[12]\d|3[01])',
               lambda m: f"{m.group(1)}{_map_val(m.group(2),'DD',mapping)}", p)
    p = re.sub(r'(?i)(year=)((?:19|20)\d{2})',
               lambda m: f"{m.group(1)}{_map_val(m.group(2),'YYYY',mapping)}", p)

    # 5) standalone 4-digit year
    p = re.sub(r'(?<![A-Za-z])((?:19|20)\d{2})(?![A-Za-z])',
               lambda m: _map_val(m.group(1),'YYYY',mapping), p)

    # 6) standalone MM and DD
    p = re.sub(r'(?<![A-Za-z0-9])(0[1-9]|1[0-2])(?![A-Za-z0-9])',
               lambda m: _map_val(m.group(1),'MM',mapping), p)
    p = re.sub(r'(?<![A-Za-z0-9])(0[1-9]|[12]\d|3[01])(?![A-Za-z0-9])',
               lambda m: _map_val(m.group(1),'DD',mapping), p)

    # 7) final fallback: standalone 2-digit groups -> YY
    p = re.sub(r'(?<![A-Za-z0-9])(\d{2})(?![A-Za-z0-9])',
               lambda m: _map_val(m.group(1),'YY',mapping), p)

    return p, mapping


# helper functions
def _map_val(val: str, token: str, mapping: dict) -> str:
    mapping[val] = token
    return token

def _map_seq(match, tokens, mapping: dict) -> str:
    groups = match.groups()
    for val, token in zip(groups, tokens):
        mapping[val] = token
    return "/".join(tokens)


####
import re

def reverse_date_path_v4(path: str) -> str:
    """
    Robust reverse function that:
     - detects YYYYMMDD, YYYYMM, YYYY (with plausible years 1900-2099)
     - detects yyyy/mm/dd and yy/mm/dd forms
     - recognizes month= / day= / year= context
     - avoids replacing digits embedded in alphabetic words (e.g. COMMUNICATION)
     - avoids wrongly replacing 2-digit groups inside longer numeric sequences
    """
    p = path

    # 1) contiguous numeric formats (longest-first); require plausible year (19|20) where appropriate
    p = re.sub(r'(?<![A-Za-z])((?:19|20)\d{6})(?![A-Za-z])', 'YYYYMMDD', p)
    p = re.sub(r'(?<![A-Za-z])((?:19|20)\d{4})(?![A-Za-z])', 'YYYYMM', p)

    # 2) yyyy/mm/dd or yyyy-mm-dd or yyyy_mm_dd  -> YYYY/MM/DD
    p = re.sub(
        r'(?<![A-Za-z])((?:19|20)\d{2})[\/\-_](0[1-9]|1[0-2])[\/\-_](0[1-9]|[12]\d|3[01])(?![A-Za-z])',
        'YYYY/MM/DD',
        p
    )
    # 3) yy/mm/dd -> YY/MM/DD
    p = re.sub(
        r'(?<![A-Za-z])(\d{2})[\/\-_](0[1-9]|1[0-2])[\/\-_](0[1-9]|[12]\d|3[01])(?![A-Za-z])',
        'YY/MM/DD',
        p
    )

    # 4) context-aware key=value replacements (case-insensitive)
    p = re.sub(r'(?i)(month=)(0[1-9]|1[0-2])', r'\1MM', p)
    p = re.sub(r'(?i)(day=)(0[1-9]|[12]\d|3[01])', r'\1DD', p)
    p = re.sub(r'(?i)(year=)(19|20)\d{2}', r'\1YYYY', p)

    # 5) standalone 4-digit year (1900-2099) -> YYYY (avoid inside words)
    p = re.sub(r'(?<![A-Za-z])((?:19|20)\d{2})(?![A-Za-z])', 'YYYY', p)

    # 6) standalone MM and DD (01-12, 01-31) - avoid adjacent letters/digits
    p = re.sub(r'(?<![A-Za-z0-9])(0[1-9]|1[0-2])(?![A-Za-z0-9])', 'MM', p)
    p = re.sub(r'(?<![A-Za-z0-9])(0[1-9]|[12]\d|3[01])(?![A-Za-z0-9])', 'DD', p)

    # 7) final fallback: standalone 2-digit groups -> YY (strict boundaries: not adjacent to letters/digits)
    p = re.sub(r'(?<![A-Za-z0-9])(\d{2})(?![A-Za-z0-9])', 'YY', p)

    return p



#####
#new smarter way
import re

def reverse_date_path(resolved_path):
    """
    Reverse date placeholders (YYYY, YY, MM, DD, YYYYMM, YYYYMMDD)
    with context-aware detection.
    """
    path = resolved_path

    # Step 1: Longest → shortest direct matches
    replacements = [
        (r'(?<!\d)(\d{8})(?!\d)', 'YYYYMMDD'),  # e.g. 20231115
        (r'(?<!\d)(\d{6})(?!\d)', 'YYYYMM'),    # e.g. 202311
        (r'(?<!\d)(\d{4})(?!\d)', 'YYYY'),      # e.g. 2023
    ]
    for pattern, placeholder in replacements:
        path = re.sub(pattern, placeholder, path)

    # Step 2: Context-aware replacements
    path = re.sub(r'(?i)(month=)(\d{2})', r'\1MM', path)  # month=11 → month=MM
    path = re.sub(r'(?i)(day=)(\d{2})', r'\1DD', path)    # day=15 → day=DD
    path = re.sub(r'(?i)(year=)(\d{4})', r'\1YYYY', path) # year=2023 → year=YYYY

    # Step 3: Delimiter-based patterns (e.g. 23/11/15)
    path = re.sub(r'(?<!\d)(\d{2})[/-](\d{2})[/-](\d{2})(?!\d)',
                  r'YY/MM/DD', path)

    # Step 4: Remaining standalone 2-digits (fallback to YY)
    path = re.sub(r'(?<!\d)(\d{2})(?!\d)', 'YY', path)

    return path


####
import re

def reverse_resolve_date_path(file_path: str) -> str:
    """
    Reverse-engineer file paths by replacing detected date substrings 
    with generic placeholders (YYYY, YY, MM, DD, YYYYMM, YYYYMMDD).
    """

    # Replace YYYYMMDD first (most specific: 8 digits)
    file_path = re.sub(r'\b\d{8}\b', 'YYYYMMDD', file_path)

    # Replace YYYYMM (6 digits) but not overlapping with YYYYMMDD
    file_path = re.sub(r'\b\d{6}\b', 'YYYYMM', file_path)

    # Replace YYYY (4 digits, usually years between 1900–2099)
    file_path = re.sub(r'\b(19|20)\d{2}\b', 'YYYY', file_path)

    # Replace YY (2 digits) carefully, avoid touching parts of larger numbers
    file_path = re.sub(r'\b\d{2}\b', 'YY', file_path)

    # Replace standalone MM (01–12)
    file_path = re.sub(r'\b(0[1-9]|1[0-2])\b', 'MM', file_path)

    # Replace standalone DD (01–31)
    file_path = re.sub(r'\b(0[1-9]|[12]\d|3[01])\b', 'DD', file_path)

    return file_path



#####
from google.cloud import bigquery_storage_v1
from google.cloud.bigquery_storage_v1 import types
from google.protobuf.json_format import ParseDict
import uuid


def insert_rows(project_id: str, dataset: str, table: str, rows: list[dict], stream_type: str = "COMMITTED"):
    """
    Insert rows into BigQuery using the Storage Write API.
    Args:
        project_id (str): GCP Project ID
        dataset (str): BigQuery Dataset
        table (str): BigQuery Table
        rows (list[dict]): List of rows to insert
        stream_type (str): "COMMITTED" (streaming) or "PENDING" (batch)
    """

    client = bigquery_storage_v1.BigQueryWriteClient()
    table_path = client.table_path(project_id, dataset, table)

    # Create a write stream
    write_stream = types.WriteStream(type_=getattr(types.WriteStream.Type, stream_type))
    stream = client.create_write_stream(parent=table_path, write_stream=write_stream)

    # Build the AppendRowsRequest
    proto_schema = client.get_write_stream(name=stream.name).table_schema
    request_template = types.AppendRowsRequest(
        write_stream=stream.name,
        proto_rows=types.AppendRowsRequest.ProtoData(
            writer_schema=types.ProtoSchema(proto_descriptor=proto_schema.proto_descriptor)
        ),
    )

    # Fill in rows
    proto_rows = types.ProtoRows()
    for row in rows:
        # Add deduplication ID
        row["_insert_id"] = str(uuid.uuid4())
        proto_row = ParseDict(row, types.ProtoRows().rows.add())  # convert dict -> proto
        proto_rows.rows.append(proto_row)

    # Send request
    request = types.AppendRowsRequest(
        write_stream=stream.name,
        proto_rows=types.AppendRowsRequest.ProtoData(rows=proto_rows),
    )
    response = client.append_rows(iter([request]))

    # Finalize batch streams
    if stream_type == "PENDING":
        client.finalize_write_stream(name=stream.name)

    return f"✅ Inserted {len(rows)} rows into {dataset}.{table} with {stream_type} stream."


# -------------------
# Example usage
# -------------------
if __name__ == "__main__":
    PROJECT = "your_project"
    DATASET = "your_dataset"
    TABLE = "your_table"

    test_rows = [
        {"id": 1, "name": "Alice"},
        {"id": 2, "name": "Bob"},
    ]

    print(insert_rows(PROJECT, DATASET, TABLE, test_rows, stream_type="COMMITTED"))





#######
from google.cloud import bigquery_storage_v1
from google.cloud.bigquery_storage_v1 import types
import uuid


def insert_rows(project_id: str, dataset: str, table: str, rows: list[dict], stream_type: str = "COMMITTED"):
    """
    Insert rows into BigQuery using the Storage Write API (JsonStreamWriter).
    """

    client = bigquery_storage_v1.BigQueryWriteClient()
    table_path = client.table_path(project_id, dataset, table)

    # Create write stream
    write_stream = types.WriteStream()
    write_stream.type_ = getattr(types.WriteStream.Type, stream_type)
    stream = client.create_write_stream(parent=table_path, write_stream=write_stream)

    # JSON stream writer
    writer = bigquery_storage_v1.JsonStreamWriter(
        client=client,
        stream_name=stream.name,
    )

    # Add insert IDs (optional but recommended for deduplication)
    for row in rows:
        row["_insert_id"] = str(uuid.uuid4())

    # Append rows
    writer.append(rows)
    writer.close()

    if stream_type == "PENDING":
        client.finalize_write_stream(name=stream.name)

    return f"✅ Inserted {len(rows)} rows into {dataset}.{table} using {stream_type} stream."


# -----------------------
# Quick Test Run
# -----------------------
if __name__ == "__main__":
    PROJECT = "your_project"
    DATASET = "your_dataset"
    TABLE = "your_table"

    test_rows = [
        {"id": 1, "name": "Alice"},
        {"id": 2, "name": "Bob"},
    ]

    print(insert_rows(PROJECT, DATASET, TABLE, test_rows, stream_type="COMMITTED"))


#####
from google.cloud import bigquery_storage_v1
from google.api_core.retry import Retry
import json
import uuid


class BigQueryWriter:
    def __init__(self, project_id: str, dataset: str, table: str):
        self.client = bigquery_storage_v1.BigQueryWriteClient()
        self.parent = self.client.table_path(project_id, dataset, table)

    def insert_rows(self, rows: list[dict], stream_type: str = "COMMITTED"):
        """
        Insert rows into BigQuery using the Storage Write API.

        Args:
            rows (list[dict]): List of rows to insert (e.g., [{"id": 1, "name": "Alice"}]).
            stream_type (str): "COMMITTED" (default) for real-time,
                               "PENDING" for batch load.
        """
        # Create write stream (COMMITTED = visible immediately)
        write_stream = bigquery_storage_v1.WriteStream()
        write_stream.type_ = getattr(bigquery_storage_v1.WriteStream.Type, stream_type)
        stream = self.client.create_write_stream(parent=self.parent, write_stream=write_stream)

        # Prepare rows
        proto_rows = bigquery_storage_v1.types.ProtoRows()
        for row in rows:
            # Assign unique insert_id for deduplication
            row["_insert_id"] = str(uuid.uuid4())
            proto_rows.serialized_rows.append(json.dumps(row).encode("utf-8"))

        # Create request
        request = bigquery_storage_v1.AppendRowsRequest(
            write_stream=stream.name,
            proto_rows=bigquery_storage_v1.AppendRowsRequest.ProtoData(
                rows=proto_rows
            )
        )

        # Append rows with retry
        response = self.client.append_rows(
            iter([request]),
            retry=Retry(deadline=30.0, maximum=3)  # retry on transient errors
        )

        # Finalize the stream for batch loads
        if stream_type == "PENDING":
            self.client.finalize_write_stream(name=stream.name)

        return response


# -------------------
# ✅ Usage Example
# -------------------
if __name__ == "__main__":
    writer = BigQueryWriter("your_project", "your_dataset", "your_table")

    # Real-time streaming insert
    rows = [
        {"id": 1, "name": "Alice"},
        {"id": 2, "name": "Bob"}
    ]
    response = writer.insert_rows(rows, stream_type="COMMITTED")
    print("Streaming insert done:", response)

    # Batch insert (not visible until committed later)
    batch_rows = [
        {"id": 3, "name": "Charlie"},
        {"id": 4, "name": "David"}
    ]
    response = writer.insert_rows(batch_rows, stream_type="PENDING")
    print("Batch insert done:", response)


#####
WITH parent AS (
  SELECT
    bucket_name,
    file_path AS original_path
  FROM `project.dataset.parent_logs`
),
resolved AS (
  SELECT
    bucket_name,
    resolved_path
  FROM `project.dataset.resolved_paths`
)

SELECT
  p.bucket_name,
  p.original_path,
  r.resolved_path
FROM parent p
JOIN resolved r
  ON p.bucket_name = r.bucket_name
 AND REGEXP_REPLACE(
       p.original_path,
       r'YYYYMMDD|yyyyMMdd|YYYYMM|yyyyMM|YYYY|yyyy|YY|yy|MM|mm|DD|dd',
       ''
     ) = REGEXP_REPLACE(
       r.resolved_path,
       r'[0-9]{8}|[0-9]{6}|[0-9]{4}|[0-9]{2}',
       ''
     );


#####
import re
from datetime import datetime

def resolve_date_path(file_path: str, target_date: datetime = None) -> str:
    """
    Replace date placeholders safely, avoiding replacements embedded within alphabetic words.
    Supports both uppercase and lowercase tokens:
      YYYY, YY, MM, DD
      yyyy, yy, mm, dd
    Also supports combined tokens like YYYYMMDD or yyyymmdd.
    """
    if target_date is None:
        target_date = datetime.now()

    # Replacement tokens ordered longest-first
    replacements = [
        ("YYYYMMDD", target_date.strftime("%Y%m%d")),
        ("YYYYMM",   target_date.strftime("%Y%m")),
        ("YYYY",     target_date.strftime("%Y")),
        ("YY",       target_date.strftime("%y")),
        ("MM",       target_date.strftime("%m")),
        ("DD",       target_date.strftime("%d")),

        ("yyyyMMdd", target_date.strftime("%Y%m%d")),  # lowercase full
        ("yyyyMM",   target_date.strftime("%Y%m")),
        ("yyyy",     target_date.strftime("%Y")),
        ("yy",       target_date.strftime("%y")),
        ("mm",       target_date.strftime("%m")),
        ("dd",       target_date.strftime("%d")),
    ]

    result = file_path
    for placeholder, replacement in replacements:
        # negative lookbehind/lookahead for alphabetic characters only
        # ensures tokens inside words like COMMUNICATION won't be replaced
        pattern = rf"(?<![A-Za-z]){re.escape(placeholder)}(?![A-Za-z])"
        result = re.sub(pattern, replacement, result)

    return result


------
import re
from datetime import datetime

def resolve_date_path(file_path, target_date=None):
    """
    Safely resolve date placeholders (YYYY, YY, MM, DD, YYYYMM, YYYYMMDD)
    in a file path without replacing random substrings like 'COMMUNICATION'.
    """
    if target_date is None:
        target_date = datetime.now()

    # Ordered longest → shortest to avoid partial replacements
    format_replacements = [
        (r'YYYYMMDD', target_date.strftime('%Y%m%d')),
        (r'YYYYMM', target_date.strftime('%Y%m')),
        (r'YYYY', target_date.strftime('%Y')),
        (r'YY', target_date.strftime('%y')),
        (r'MM', target_date.strftime('%m')),
        (r'DD', target_date.strftime('%d'))
    ]

    # Replace only when they appear as clear tokens or sequences
    for pattern, replacement in format_replacements:
        file_path = re.sub(pattern, replacement, file_path)

    return file_path

TEST_DATE = datetime(2023, 11, 15)

print(resolve_date_path("data/YYYY/file.json", TEST_DATE))    
# → data/2023/file.json

print(resolve_date_path("logs/YYYYMM/app.log", TEST_DATE))   
# → logs/202311/app.log

print(resolve_date_path("exports/YYYYMMDD/data.csv", TEST_DATE))   
# → exports/20231115/data.csv

print(resolve_date_path("archive/YYYY/MM/DD/backup.zip", TEST_DATE))   
# → archive/2023/11/15/backup.zip

print(resolve_date_path("COMMUNICATION_test.txt", TEST_DATE))   
# → COMMUNICATION_test.txt   ✅ untouched

print(resolve_date_path("X_YY_MM_DD_Y", TEST_DATE))  
# → X_23_11_15_Y

print(resolve_date_path("1YYYY2MM3DD4", TEST_DATE))  
# → 1202311154   ✅ fixed




#####
WITH parsed AS (
  SELECT
    resource.labels.bucket_name AS bucket_name,
    -- Extract file_name without generation number
    REGEXP_EXTRACT(protoPayload.resourceName, r'/objects/([^#]+)') AS file_name,
    timestamp AS create_ts
  FROM
    `project.gcs_logs.cloudaudit_googleapis_com_data_access`
  WHERE
    protoPayload.methodName = "storage.objects.create"
    AND protoPayload.resourceName IS NOT NULL
)

SELECT
  bucket_name,
  create_date,
  COUNT(1) AS unique_file_count_first_seen
FROM (
  SELECT
    bucket_name,
    file_name,
    DATE(create_ts) AS create_date,
    ROW_NUMBER() OVER (
      PARTITION BY bucket_name, file_name
      ORDER BY create_ts
    ) AS rn
  FROM parsed
  WHERE file_name IS NOT NULL
)
WHERE rn = 1
GROUP BY bucket_name, create_date
ORDER BY bucket_name, create_date;
