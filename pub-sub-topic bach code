from google.cloud import storage
from google.auth.transport.requests import AuthorizedSession
import google.auth
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

def get_custom_storage_client(pool_size=100):
    # Get default credentials
    credentials, _ = google.auth.default()

    # Create a requests session
    session = requests.Session()

    # Configure HTTPAdapter with larger connection pool
    adapter = HTTPAdapter(
        pool_connections=pool_size,
        pool_maxsize=pool_size,
        max_retries=Retry(total=3, backoff_factor=0.3)
    )

    # Mount adapter to HTTP and HTTPS
    session.mount("https://", adapter)
    session.mount("http://", adapter)

    # AuthorizedSession wraps the session with credentials
    authed_session = AuthorizedSession(credentials, session=session)

    # Pass the session into the BigQuery client
    return storage.Client(_http=authed_session)

# Example usage
client = get_custom_storage_client(pool_size=50)



######
import base64
import json
import os
from google.cloud import bigquery, pubsub_v1
from google.api_core.exceptions import NotFound
import functions_framework

# Initialize clients globally (they're thread-safe)
bq_client = bigquery.Client()
subscriber = pubsub_v1.SubscriberClient()

# Configurable parameters
PROJECT_ID = os.getenv('GOOGLE_CLOUD_PROJECT')
DATASET_ID = os.getenv('BQ_DATASET', 'file_uploads')
TABLE_ID = os.getenv('BQ_TABLE', 'uploaded_files')
SUBSCRIPTION_ID = os.getenv('PUBSUB_SUBSCRIPTION', 'file-upload-sub')

@functions_framework.http
def process_uploaded_files(request):
    """HTTP-triggered Gen 2 function to process Pub/Sub messages."""
    try:
        # 1. Pull messages from Pub/Sub
        subscription_path = subscriber.subscription_path(PROJECT_ID, SUBSCRIPTION_ID)
        response = subscriber.pull(
            subscription=subscription_path,
            max_messages=10,  # Process up to 10 messages at once
            timeout=5.0
        )
        
        if not response.received_messages:
            return {"status": "success", "message": "No messages to process"}, 200

        ack_ids = []
        processed_messages = []

        # 2. Process each message
        for msg in response.received_messages:
            try:
                file_data = parse_pubsub_message(msg.message)
                insert_into_bigquery(file_data)
                
                # Collect successfully processed messages
                ack_ids.append(msg.ack_id)
                processed_messages.append({
                    "file_name": file_data.get("name"),
                    "status": "processed"
                })
            except Exception as e:
                print(f"Failed to process message: {str(e)}")
                continue

        # 3. Acknowledge successful messages
        if ack_ids:
            subscriber.acknowledge(
                subscription=subscription_path,
                ack_ids=ack_ids
            )

        return {
            "status": "success",
            "processed": len(processed_messages),
            "messages": processed_messages
        }, 200

    except Exception as e:
        print(f"Error processing messages: {str(e)}")
        return {"status": "error", "message": str(e)}, 500

def parse_pubsub_message(message):
    """Parse Pub/Sub message data from file upload notification."""
    try:
        # Decode base64 message data if needed
        if message.data:
            data = base64.b64decode(message.data).decode('utf-8')
            event = json.loads(data)
            
            # Extract relevant file information
            return {
                "name": event.get("name"),
                "bucket": event.get("bucket"),
                "size": event.get("size"),
                "time_created": event.get("timeCreated"),
                "content_type": event.get("contentType"),
                "processed_at": bigquery.ScalarQueryParameter(
                    "processed_at", "TIMESTAMP", bigquery.Client().query(
                        "SELECT CURRENT_TIMESTAMP()").result().one()[0]
                )
            }
        raise ValueError("Empty message data")
    except Exception as e:
        print(f"Error parsing message: {str(e)}")
        raise

def insert_into_bigquery(file_data):
    """Insert file metadata into BigQuery table."""
    table_ref = f"{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}"
    
    # Ensure table exists
    try:
        bq_client.get_table(table_ref)
    except NotFound:
        create_upload_table(table_ref)
    
    # Prepare and execute insert
    errors = bq_client.insert_rows_json(
        table_ref,
        [file_data]
    )
    
    if errors:
        raise RuntimeError(f"BigQuery insert errors: {errors}")

def create_upload_table(table_ref):
    """Create the files table if it doesn't exist."""
    schema = [
        bigquery.SchemaField("name", "STRING", mode="REQUIRED"),
        bigquery.SchemaField("bucket", "STRING", mode="REQUIRED"),
        bigquery.SchemaField("size", "INT64"),
        bigquery.SchemaField("time_created", "TIMESTAMP"),
        bigquery.SchemaField("content_type", "STRING"),
        bigquery.SchemaField("processed_at", "TIMESTAMP", mode="REQUIRED"),
    ]
    
    table = bigquery.Table(table_ref, schema=schema)
    bq_client.create_table(table)
    print(f"Created table {table_ref}")

#####
Step 1: File Upload Notification to Pub/Sub
Ensure your Cloud Storage bucket is configured to send notifications to the Pub/Sub topic.

Step 2: Cloud Function to Collect Messages
This Cloud Function will receive file notifications, accumulate them into batches, and trigger batch processing.

python
Copy code
import os
import json
import time
from google.cloud import pubsub_v1
from google.cloud import firestore

# Configuration
BATCH_SIZE = 50
BATCH_TIMEOUT = 60  # seconds
PROJECT_ID = os.getenv('GCP_PROJECT')
COLLECTION_NAME = 'file_batches'

# Initialize Firestore
db = firestore.Client()

# Initialize Pub/Sub
publisher = pubsub_v1.PublisherClient()
batch_topic = 'projects/{project_id}/topics/{topic_name}'.format(
    project_id=PROJECT_ID,
    topic_name='batch-processing-topic'
)

def process_batch(batch):
    # Implement your batch processing logic here
    print("Processing batch:", batch)
    # For example, store metadata in BigQuery or other actions
    pass

def accumulate_messages(message):
    # Store message in Firestore
    doc_ref = db.collection(COLLECTION_NAME).document(message['id'])
    doc_ref.set({
        'metadata': message,
        'timestamp': time.time()
    })

def clean_old_batches():
    # Remove old batch documents from Firestore
    threshold = time.time() - BATCH_TIMEOUT
    docs = db.collection(COLLECTION_NAME).where('timestamp', '<', threshold).stream()
    for doc in docs:
        doc.reference.delete()

def cloud_function_handler(event, context):
    message = json.loads(base64.b64decode(event['data']).decode('utf-8'))
    
    # Accumulate the message
    accumulate_messages(message)
    
    # Clean up old batches
    clean_old_batches()

    # Retrieve all accumulated messages
    docs = db.collection(COLLECTION_NAME).stream()
    batch = [doc.to_dict() for doc in docs]

    if len(batch) >= BATCH_SIZE:
        # Trigger batch processing
        process_batch(batch)
        
        # Publish a message to another Pub/Sub topic for batch processing
        publisher.publish(batch_topic, data=json.dumps(batch).encode('utf-8'))
        
        # Clear accumulated messages after processing
        for doc in docs:
            doc.reference.delete()
Step 3: Cloud Function to Process Batches
This Cloud Function will process the batches of file metadata.

python
Copy code
import json
from google.cloud import bigquery

# Initialize BigQuery
client = bigquery.Client()

def process_batch(batch):
    # Implement schema validation and metadata storage logic here
    print("Processing batch:", batch)
    
    # Example: Insert metadata into BigQuery
    table_id = 'your-project.your_dataset.your_table'
    rows_to_insert = [{'file_metadata': json.dumps(item)} for item in batch]
    
    errors = client.insert_rows_json(table_id, rows_to_insert)
    if errors:
        print("Errors occurred while inserting rows:", errors)
    else:
        print("Batch processed successfully")

def batch_processing_function(event, context):
    # Decode Pub/Sub message
    batch = json.loads(base64.b64decode(event['data']).decode('utf-8'))
    
    # Process the batch
    process_batch(batch)
Additional Steps
Deploy Cloud Functions: Deploy both functions using the gcloud command:

sh
Copy code
gcloud functions deploy cloud_function_handler \
  --runtime python39 \
  --trigger-topic your-pubsub-topic \
  --entry-point cloud_function_handler \
  --project your-gcp-project

gcloud functions deploy batch_processing_function \
  --runtime python39 \
  --trigger-topic batch-processing-topic \
  --entry-point batch_processing_function \
  --project your-gcp-project
Set Up IAM Permissions: Ensure the Cloud Functions have the necessary IAM permissions to access Pub/Sub, Firestore, and BigQuery.

Testing and Monitoring: Test the functions with sample messages and monitor their performance and logs in the Google Cloud Console.

This code is a starting point and may need to be adapted based on your specific requirements and architecture.
