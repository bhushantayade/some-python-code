from google.cloud import bigquery
from google.api_core.exceptions import GoogleAPICallError, RetryError
from google.api_core.retry import Retry
import functions_framework
from typing import Optional, Union, List, Dict, Any
from concurrent.futures import ThreadPoolExecutor
import threading
import json

class BigQueryConnectionManager:
    """Thread-safe BigQuery connection handler with concurrent processing support."""
    
    _instance = None
    _lock = threading.Lock()
    
    def __new__(cls, *args, **kwargs):
        """Singleton pattern to avoid multiple instances."""
        with cls._lock:
            if cls._instance is None:
                cls._instance = super().__new__(cls)
                cls._instance._init_connection(*args, **kwargs)
            return cls._instance
    
    def _init_connection(self, 
                        project_id: Optional[str] = None,
                        location: Optional[str] = None,
                        default_dataset: Optional[str] = None):
        """Initialize connection parameters (called once)."""
        self._project_id = project_id
        self._location = location
        self._default_dataset = default_dataset
        self._client = None
        self._client_lock = threading.Lock()
        self._executor = ThreadPoolExecutor()
    
    @property
    def client(self) -> bigquery.Client:
        """Thread-safe lazy loading of BigQuery client."""
        if self._client is None:
            with self._client_lock:
                if self._client is None:  # Double-check locking
                    self._client = bigquery.Client(
                        project=self._project_id,
                        location=self._location
                    )
        return self._client
    
    def close(self) -> None:
        """Safely close the connection and cleanup resources."""
        with self._client_lock:
            if self._client is not None:
                self._client.close()
                self._client = None
            if hasattr(self, '_executor'):
                self._executor.shutdown(wait=True)
    
    def test_connection(self) -> bool:
        """Test connection with thread-safe operation."""
        try:
            # Use a simple metadata operation to test connection
            with self._client_lock:
                self.client.list_datasets(max_results=1)
            return True
        except GoogleAPICallError:
            return False
    
    def _execute_query_threadsafe(
        self,
        query: str,
        params: Optional[Union[List, Dict]] = None,
        retry: Optional[Retry] = None
    ) -> List[Dict[str, Any]]:
        """Internal thread-safe query execution method."""
        job_config = bigquery.QueryJobConfig()
        if params:
            job_config.query_parameters = params
        if self._default_dataset:
            job_config.default_dataset = f"{self.client.project}.{self._default_dataset}"
        
        with self._client_lock:
            query_job = self.client.query(
                query,
                job_config=job_config,
                retry=retry
            )
            return [dict(row) for row in query_job.result()]
    
    async def execute_query_async(
        self,
        query: str,
        params: Optional[Union[List, Dict]] = None,
        retry: Optional[Retry] = None
    ) -> List[Dict[str, Any]]:
        """Execute query asynchronously for concurrent processing."""
        return await self._executor.submit(
            self._execute_query_threadsafe,
            query,
            params,
            retry
        )
    
    def process_files_concurrently(
        self,
        file_data: List[Dict[str, Any]],
        process_func: callable,
        max_workers: Optional[int] = None
    ) -> List[Any]:
        """
        Process multiple files concurrently with BigQuery operations.
        
        Args:
            file_data: List of file metadata/contents to process
            process_func: Function that takes (connection, file_data) and returns processed result
            max_workers: Maximum concurrent workers (default: ThreadPoolExecutor default)
            
        Returns:
            List of results in the same order as input
        """
        def _process_wrapper(file_item):
            try:
                return process_func(self, file_item)
            except Exception as e:
                return {"error": str(e), "file": file_item.get('filename', 'unknown')}
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            results = list(executor.map(_process_wrapper, file_data))
        
        return results

# Example Usage in Cloud Function
@functions_framework.http
def process_files_with_bigquery(request):
    """HTTP Cloud Function for concurrent file processing with BigQuery."""
    try:
        # Initialize connection manager (singleton)
        bq_manager = BigQueryConnectionManager(
            project_id="your-project-id",
            location="US",
            default_dataset="your_dataset"
        )
        
        # Example file processing function
        def process_file(conn, file_item):
            """Example processing function for each file."""
            # 1. Parse file content
            data = json.loads(file_item['content'])
            
            # 2. Insert into staging table
            conn.execute_query_async(
                "INSERT INTO temp_staging (data) VALUES (@data)",
                params=[bigquery.ScalarQueryParameter("data", "STRING", json.dumps(data))]
            )
            
            # 3. Transform data
            transformed = conn.execute_query_async(
                "CALL transform_data(@file_id)",
                params=[bigquery.ScalarQueryParameter("file_id", "STRING", file_item['id'])]
            )
            
            return {"file_id": file_item['id'], "status": "processed", "rows": len(transformed)}
        
        # Get files from request (mock example)
        files_to_process = request.get_json().get('files', [])
        
        # Process files concurrently
        results = bq_manager.process_files_concurrently(
            file_data=files_to_process,
            process_func=process_file,
            max_workers=5  # Adjust based on your needs
        )
        
        return {
            "status": "success",
            "processed_files": len(results),
            "results": results
        }, 200
        
    except Exception as e:
        return {
            "status": "error",
            "message": str(e)
        }, 500

#####
from google.cloud import storage

def copy_files_to_new_bucket(file_paths: list, destination_bucket_name: str):
    storage_client = storage.Client()

    for full_path in file_paths:
        parts = full_path.split('/', 1)
        if len(parts) != 2:
            print(f"Invalid file path format: {full_path}")
            continue

        source_bucket_name, blob_path = parts

        source_bucket = storage_client.bucket(source_bucket_name)
        source_blob = source_bucket.blob(blob_path)

        destination_bucket = storage_client.bucket(destination_bucket_name)
        destination_blob = destination_bucket.blob(blob_path)

        destination_blob.rewrite(source_blob)
        print(f"✅ Copied: {full_path} → gs://{destination_bucket_name}/{blob_path}")

# Example test case
if __name__ == "__main__":
    test_file_paths = [
        "source-bucket-123/folder1/test1.txt",
        "source-bucket-123/folder2/test2.csv"
    ]
    destination = "destination-bucket-456"
    copy_files_to_new_bucket(test_file_paths, destination)




gsutil -m cp -r gs://source-bucket-name/* gs://destination-bucket-name/



from google.cloud import storage

def copy_all_files(source_bucket_name, destination_bucket_name, prefix=""):
    client = storage.Client()

    source_bucket = client.bucket(source_bucket_name)
    destination_bucket = client.bucket(destination_bucket_name)

    blobs = client.list_blobs(source_bucket, prefix=prefix)

    for blob in blobs:
        source_blob = source_bucket.blob(blob.name)
        destination_blob = destination_bucket.blob(blob.name)
        
        destination_blob.rewrite(source_blob)
        print(f"Copied: {blob.name}")

# Example usage
copy_all_files("source-bucket-name", "destination-bucket-name")



######
import re

def get_base_name(filename):
    """
    Robust financial filename parser that correctly handles:
    - Standard prefix patterns (INST_REPORT_YYYYMMDD)
    - Date-first patterns (YYYYMMDD_INST_REPORT)
    - Mixed delimiters (._-)
    - Version tags (_v2, .v3)
    - No-delimiter fallback (INSTYYYYMMDD)
    """
    # Preprocess filename (normalize delimiters and remove spaces)
    normalized = re.sub(r'[ .-]', '_', filename.strip())
    
    # Ordered patterns (most specific to most general)
    patterns = [
        # 1. Standard prefix before date (INST_REPORT_YYYYMMDD)
        r'^([a-zA-Z0-9_]+(?:_[a-zA-Z0-9]+)*)_\d{8}(?:_\d+)?(?:\..+)?$',
        
        # 2. Dot-delimited prefix (INST.REPORT.YYYYMMDD)
        r'^([a-zA-Z0-9_]+(?:\.[a-zA-Z0-9]+)*)\.\d{8}(?:\.\d+)?(?:\..+)?$',
        
        # 3. Date-first patterns (YYYYMMDD_INST_REPORT)
        r'^\d{8}_([a-zA-Z0-9_]+(?:_[a-zA-Z0-9]+)*)(?:_v\d+)?(?:\..+)?$',
        
        # 4. No-delimiter patterns (INSTYYYYMMDD)
        r'^([a-zA-Z]+)\d{8}(?:\..+)?$',
        
        # 5. Versioned files (INST_REPORT_v2_YYYYMMDD)
        r'^([a-zA-Z0-9_]+(?:_[a-zA-Z0-9]+)*)(?:_v\d+)?_\d{8}(?:\..+)?$'
    ]
    
    for pattern in patterns:
        match = re.match(pattern, normalized)
        if match:
            base = match.group(1)
            # Clean up any residual delimiters
            base = re.sub(r'_{2,}', '_', base).rstrip('_')
            return base.replace('.', '_')  # Standardize to underscores
    
    # Final fallback - take everything before first dot
    return normalized.split('.')[0]

#####
def get_base_name(filename):
    """
    Final robust version handling:
    - All standard financial patterns
    - Date-first and date-last formats
    - Mixed delimiters (._)
    - Version tags
    - No-delimiter fallback
    """
    patterns = [
        r'^([a-zA-Z0-9_.]+?)[._]\d{8}(?:[._]\d+)?(?:\..+)?$',  # Standard prefix
        r'^\d{8}[._]([a-zA-Z0-9_.]+?)(?:[._]v\d+)?(?:\..+)?$', # Date-first
        r'^([a-zA-Z0-9]+)\d{8}(?:\..+)?$',                     # No-delimiter
        r'^([a-zA-Z0-9_.]+)$'                                   # No pattern match
    ]
    
    for pattern in patterns:
        match = re.match(pattern, filename)
        if match:
            return match.group(1)
    return filename.split('.')[0]  # Ultimate fallback


#####
from google.cloud import bigquery

client = bigquery.Client()

query = """
WITH extracted_files AS (
  SELECT 
    file_name,
    file_size,
    create_ts,
    REGEXP_EXTRACT(file_name, r'^(.*)_\\d+_\\d+$') AS base_name
  FROM `your_project.your_dataset.file_size_table`
  WHERE REGEXP_CONTAINS(file_name, r'^category_distribute_\\d+_\\d+$')
),
latest_files AS (
  SELECT 
    base_name,
    MAX(create_ts) AS latest_create_ts
  FROM extracted_files
  GROUP BY base_name
)
SELECT 
  f.file_name,
  f.file_size,
  f.create_ts,
  f.base_name
FROM extracted_files f
JOIN latest_files l
  ON f.base_name = l.base_name AND f.create_ts = l.latest_create_ts
ORDER BY f.create_ts DESC
"""

query_job = client.query(query)
results = query_job.result()

for row in results:
    print(f"Base: {row.base_name}, File: {row.file_name}, Size: {row.file_size}, Date: {row.create_ts}")

